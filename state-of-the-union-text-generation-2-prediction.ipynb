{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the Union - Part 2 - Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clinton\n",
    "MODEL_NAME = 'sotu-clinton'\n",
    "MODEL_VERSION = '2'\n",
    "MAX_SEQUENCE_LEN = 284 # the following is obtained from the training\n",
    "\n",
    "## obama\n",
    "# MODEL_NAME = 'sotu-obama'\n",
    "# MODEL_VERSION = '1'\n",
    "# MAX_SEQUENCE_LEN = 129 # the following is obtained from the training\n",
    "\n",
    "## trump\n",
    "# MODEL_NAME = 'sotu-trump'\n",
    "# MODEL_VERSION = '1'\n",
    "# MAX_SEQUENCE_LEN = 157 # the following is obtained from the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculated from above\n",
    "VOCAB_FILE = 'tokenizer-vocabulary/' + MODEL_NAME + '-vocab.json'\n",
    "MODEL_FILE = 'models/'  + MODEL_NAME + '-model-' + MODEL_VERSION + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version : 2.3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print ('tensorflow version :', tf.__version__)\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-GPU Debug\n",
    "The following block tests if TF is running on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## This block is to tweak TF running on GPU\n",
    "## You may comment this out, if you are not using GPU\n",
    "\n",
    "## ---- start Memory setting ----\n",
    "## Ask TF not to allocate all GPU memory at once.. allocate as needed\n",
    "## Without this the execution will fail with \"failed to initialize algorithm\" error\n",
    "\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n",
    "## ---- end Memory setting ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step - Load Tokenizer from Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer: num_uniq_words : 4526\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "with open(VOCAB_FILE) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print ('tokenizer: num_uniq_words :', total_words)\n",
    "\n",
    "## Create reverse Index for lookup\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {v:k for (k,v) in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num words : 4526\n",
      "\n",
      "Some random word mappings : \n",
      "{'advance': 724,\n",
      " 'every': 45,\n",
      " 'explore': 1447,\n",
      " 'hearts': 1406,\n",
      " 'improvement': 2987,\n",
      " 'matters': 1563,\n",
      " 'occupational': 2868,\n",
      " 'rather': 3134,\n",
      " 'teams': 4310,\n",
      " 'warn': 3905}\n",
      "\n",
      "Top-N words:\n",
      "[('the', 2539),\n",
      " ('to', 2231),\n",
      " ('and', 1761),\n",
      " ('of', 1295),\n",
      " ('we', 1224),\n",
      " ('a', 934),\n",
      " ('in', 922),\n",
      " ('our', 901),\n",
      " ('that', 717),\n",
      " ('i', 674)]\n"
     ]
    }
   ],
   "source": [
    "## Basic info\n",
    "from  collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "def sample_from_dict(d, sample=10):\n",
    "    import random\n",
    "    \n",
    "    keys = random.sample(list(d), sample)\n",
    "    values = [d[k] for k in keys]\n",
    "    return dict(zip(keys, values))\n",
    "\n",
    "print ('total num words :', len(tokenizer.word_index)+1)\n",
    "print ('\\nSome random word mappings : ')\n",
    "pprint (sample_from_dict(tokenizer.word_index))\n",
    "\n",
    "\n",
    "counter = Counter(tokenizer.word_counts)\n",
    "print ('\\nTop-N words:')\n",
    "pprint(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index[\"america\"] : 38\n",
      "index2word [38] : america\n"
     ]
    }
   ],
   "source": [
    "## Doing sample lookup\n",
    "\n",
    "# change the word to : america, congress, court, citizen\n",
    "word = 'america'\n",
    "\n",
    "idx = word2index[word]\n",
    "print ('word2index[\"{}\"] : {}'.format(word, idx))\n",
    "print ('index2word [{}] : {}'.format(idx, index2word[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step  - Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'models/sotu-clinton-model-2.h5',  size = 14.2 MB\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "model_size_in_bytes = os.path.getsize(MODEL_FILE)\n",
    "\n",
    "\n",
    "model = load_model(MODEL_FILE)\n",
    "\n",
    "print (\"Loaded model '{}',  size = {:,.1f} MB\".format(MODEL_FILE, \n",
    "                                    model_size_in_bytes / (1024*1024) ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input text : hello america\n",
      "0 token_list: [1, 38]\n",
      "0 predicted_idx : 6\n",
      "0 output_word : we\n",
      "0 output_text: hello america we\n",
      "\n",
      "1 input text : hello america we\n",
      "1 token_list: [1, 38, 6]\n",
      "1 predicted_idx : 29\n",
      "1 output_word : must\n",
      "1 output_text: hello america we must\n",
      "\n",
      "2 input text : hello america we must\n",
      "2 token_list: [1, 38, 6, 29]\n",
      "2 predicted_idx : 26\n",
      "2 output_word : be\n",
      "2 output_text: hello america we must be\n",
      "\n",
      "3 input text : hello america we must be\n",
      "3 token_list: [1, 38, 6, 29, 26]\n",
      "3 predicted_idx : 7\n",
      "3 output_word : a\n",
      "3 output_text: hello america we must be a\n",
      "\n",
      "4 input text : hello america we must be a\n",
      "4 token_list: [1, 38, 6, 29, 26, 7]\n",
      "4 predicted_idx : 57\n",
      "4 output_word : world\n",
      "4 output_text: hello america we must be a world\n",
      "\n",
      "5 input text : hello america we must be a world\n",
      "5 token_list: [1, 38, 6, 29, 26, 7, 57]\n",
      "5 predicted_idx : 4\n",
      "5 output_word : and\n",
      "5 output_text: hello america we must be a world and\n",
      "\n",
      "6 input text : hello america we must be a world and\n",
      "6 token_list: [1, 38, 6, 29, 26, 7, 57, 4]\n",
      "6 predicted_idx : 6\n",
      "6 output_word : we\n",
      "6 output_text: hello america we must be a world and we\n",
      "\n",
      "7 input text : hello america we must be a world and we\n",
      "7 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6]\n",
      "7 predicted_idx : 29\n",
      "7 output_word : must\n",
      "7 output_text: hello america we must be a world and we must\n",
      "\n",
      "8 input text : hello america we must be a world and we must\n",
      "8 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29]\n",
      "8 predicted_idx : 26\n",
      "8 output_word : be\n",
      "8 output_text: hello america we must be a world and we must be\n",
      "\n",
      "9 input text : hello america we must be a world and we must be\n",
      "9 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26]\n",
      "9 predicted_idx : 7\n",
      "9 output_word : a\n",
      "9 output_text: hello america we must be a world and we must be a\n",
      "\n",
      "10 input text : hello america we must be a world and we must be a\n",
      "10 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7]\n",
      "10 predicted_idx : 57\n",
      "10 output_word : world\n",
      "10 output_text: hello america we must be a world and we must be a world\n",
      "\n",
      "11 input text : hello america we must be a world and we must be a world\n",
      "11 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57]\n",
      "11 predicted_idx : 4\n",
      "11 output_word : and\n",
      "11 output_text: hello america we must be a world and we must be a world and\n",
      "\n",
      "12 input text : hello america we must be a world and we must be a world and\n",
      "12 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4]\n",
      "12 predicted_idx : 6\n",
      "12 output_word : we\n",
      "12 output_text: hello america we must be a world and we must be a world and we\n",
      "\n",
      "13 input text : hello america we must be a world and we must be a world and we\n",
      "13 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6]\n",
      "13 predicted_idx : 29\n",
      "13 output_word : must\n",
      "13 output_text: hello america we must be a world and we must be a world and we must\n",
      "\n",
      "14 input text : hello america we must be a world and we must be a world and we must\n",
      "14 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29]\n",
      "14 predicted_idx : 26\n",
      "14 output_word : be\n",
      "14 output_text: hello america we must be a world and we must be a world and we must be\n",
      "\n",
      "15 input text : hello america we must be a world and we must be a world and we must be\n",
      "15 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26]\n",
      "15 predicted_idx : 7\n",
      "15 output_word : a\n",
      "15 output_text: hello america we must be a world and we must be a world and we must be a\n",
      "\n",
      "16 input text : hello america we must be a world and we must be a world and we must be a\n",
      "16 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7]\n",
      "16 predicted_idx : 57\n",
      "16 output_word : world\n",
      "16 output_text: hello america we must be a world and we must be a world and we must be a world\n",
      "\n",
      "17 input text : hello america we must be a world and we must be a world and we must be a world\n",
      "17 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57]\n",
      "17 predicted_idx : 4\n",
      "17 output_word : and\n",
      "17 output_text: hello america we must be a world and we must be a world and we must be a world and\n",
      "\n",
      "18 input text : hello america we must be a world and we must be a world and we must be a world and\n",
      "18 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4]\n",
      "18 predicted_idx : 2\n",
      "18 output_word : the\n",
      "18 output_text: hello america we must be a world and we must be a world and we must be a world and the\n",
      "\n",
      "19 input text : hello america we must be a world and we must be a world and we must be a world and the\n",
      "19 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2]\n",
      "19 predicted_idx : 57\n",
      "19 output_word : world\n",
      "19 output_text: hello america we must be a world and we must be a world and we must be a world and the world\n",
      "\n",
      "20 input text : hello america we must be a world and we must be a world and we must be a world and the world\n",
      "20 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57]\n",
      "20 predicted_idx : 5\n",
      "20 output_word : of\n",
      "20 output_text: hello america we must be a world and we must be a world and we must be a world and the world of\n",
      "\n",
      "21 input text : hello america we must be a world and we must be a world and we must be a world and the world of\n",
      "21 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5]\n",
      "21 predicted_idx : 2\n",
      "21 output_word : the\n",
      "21 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the\n",
      "\n",
      "22 input text : hello america we must be a world and we must be a world and we must be a world and the world of the\n",
      "22 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2]\n",
      "22 predicted_idx : 57\n",
      "22 output_word : world\n",
      "22 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world\n",
      "\n",
      "23 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world\n",
      "23 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57]\n",
      "23 predicted_idx : 6\n",
      "23 output_word : we\n",
      "23 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we\n",
      "\n",
      "24 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we\n",
      "24 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6]\n",
      "24 predicted_idx : 29\n",
      "24 output_word : must\n",
      "24 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must\n",
      "\n",
      "25 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must\n",
      "25 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29]\n",
      "25 predicted_idx : 26\n",
      "25 output_word : be\n",
      "25 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be\n",
      "\n",
      "26 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be\n",
      "26 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26]\n",
      "26 predicted_idx : 7\n",
      "26 output_word : a\n",
      "26 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a\n",
      "\n",
      "27 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a\n",
      "27 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7]\n",
      "27 predicted_idx : 57\n",
      "27 output_word : world\n",
      "27 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world\n",
      "\n",
      "28 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world\n",
      "28 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57]\n",
      "28 predicted_idx : 4\n",
      "28 output_word : and\n",
      "28 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and\n",
      "\n",
      "29 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and\n",
      "29 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4]\n",
      "29 predicted_idx : 6\n",
      "29 output_word : we\n",
      "29 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we\n",
      "\n",
      "30 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we\n",
      "30 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6]\n",
      "30 predicted_idx : 29\n",
      "30 output_word : must\n",
      "30 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must\n",
      "\n",
      "31 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must\n",
      "31 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29]\n",
      "31 predicted_idx : 26\n",
      "31 output_word : be\n",
      "31 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be\n",
      "\n",
      "32 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be\n",
      "32 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26]\n",
      "32 predicted_idx : 7\n",
      "32 output_word : a\n",
      "32 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a\n",
      "\n",
      "33 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a\n",
      "33 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7]\n",
      "33 predicted_idx : 57\n",
      "33 output_word : world\n",
      "33 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world\n",
      "\n",
      "34 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world\n",
      "34 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57]\n",
      "34 predicted_idx : 4\n",
      "34 output_word : and\n",
      "34 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and\n",
      "\n",
      "35 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and\n",
      "35 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4]\n",
      "35 predicted_idx : 2\n",
      "35 output_word : the\n",
      "35 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the\n",
      "\n",
      "36 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the\n",
      "36 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2]\n",
      "36 predicted_idx : 57\n",
      "36 output_word : world\n",
      "36 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world\n",
      "\n",
      "37 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world\n",
      "37 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57]\n",
      "37 predicted_idx : 5\n",
      "37 output_word : of\n",
      "37 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of\n",
      "\n",
      "38 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of\n",
      "38 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5]\n",
      "38 predicted_idx : 2\n",
      "38 output_word : the\n",
      "38 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the\n",
      "\n",
      "39 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the\n",
      "39 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2]\n",
      "39 predicted_idx : 57\n",
      "39 output_word : world\n",
      "39 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world\n",
      "\n",
      "40 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world\n",
      "40 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57]\n",
      "40 predicted_idx : 6\n",
      "40 output_word : we\n",
      "40 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we\n",
      "\n",
      "41 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we\n",
      "41 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6]\n",
      "41 predicted_idx : 29\n",
      "41 output_word : must\n",
      "41 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must\n",
      "\n",
      "42 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must\n",
      "42 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29]\n",
      "42 predicted_idx : 26\n",
      "42 output_word : be\n",
      "42 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be\n",
      "\n",
      "43 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be\n",
      "43 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26]\n",
      "43 predicted_idx : 7\n",
      "43 output_word : a\n",
      "43 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a\n",
      "\n",
      "44 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a\n",
      "44 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7]\n",
      "44 predicted_idx : 57\n",
      "44 output_word : world\n",
      "44 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world\n",
      "\n",
      "45 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world\n",
      "45 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57]\n",
      "45 predicted_idx : 4\n",
      "45 output_word : and\n",
      "45 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and\n",
      "\n",
      "46 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and\n",
      "46 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4]\n",
      "46 predicted_idx : 6\n",
      "46 output_word : we\n",
      "46 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we\n",
      "\n",
      "47 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we\n",
      "47 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6]\n",
      "47 predicted_idx : 29\n",
      "47 output_word : must\n",
      "47 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we must\n",
      "\n",
      "48 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we must\n",
      "48 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29]\n",
      "48 predicted_idx : 26\n",
      "48 output_word : be\n",
      "48 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we must be\n",
      "\n",
      "49 input text : hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we must be\n",
      "49 token_list: [1, 38, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26, 7, 57, 4, 2, 57, 5, 2, 57, 6, 29, 26, 7, 57, 4, 6, 29, 26]\n",
      "49 predicted_idx : 7\n",
      "49 output_word : a\n",
      "49 output_text: hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we must be a\n",
      "\n",
      "final competed text:\n",
      " hello america we must be a world and we must be a world and we must be a world and the world of the world we must be a world and we must be a world and the world of the world we must be a world and we must be a\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "\n",
    "\n",
    "## see text\n",
    "seed_text = \"hello america\"\n",
    "# seed_text = \"yes we can\"\n",
    "# seed_text = \"dear americans\"\n",
    "\n",
    "\n",
    "next_words = 50\n",
    "\n",
    "\n",
    "text = seed_text\n",
    "for i in range(next_words):\n",
    "    print ('{} input text : {}'.format(i,text))\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    print ('{} token_list: {}'.format(i, token_list))\n",
    "    word_list = []\n",
    "    token_list = pad_sequences([token_list], maxlen=MAX_SEQUENCE_LEN-1, padding='pre')\n",
    "    #print ('{} token_list padded: {}'.format(i, token_list))\n",
    "    \n",
    "    prediction_softmax = model.predict(token_list, verbose=0)\n",
    "    predicted_idx = [ np.argmax(p) for p in prediction_softmax][0]\n",
    "    \n",
    "    print ('{} predicted_idx : {}'.format(i, predicted_idx))\n",
    "    output_word = index2word.get(predicted_idx, \"<UNK>\")\n",
    "    print ('{} output_word : {}'.format(i, output_word))\n",
    "    text += \" \" + output_word\n",
    "    print ('{} output_text: {}'.format (i, text))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "print('final competed text:\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
